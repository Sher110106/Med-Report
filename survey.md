# Doctor Evaluation Survey Module

**Handwritten Clinical Note → SOAP Output (7-Model Comparison)**

---

## 1. Purpose & Framing (Shown Once at Survey Start)

**Objective (doctor-facing, plain language):**

> You will be shown photographs of handwritten clinical notes and multiple structured SOAP outputs generated by different AI systems.
>
> Your task is **not to diagnose**, but to judge **how accurately and safely each AI captured what was written**, and how usable it would be in real clinical documentation.

**Key instructions:**

* Judge **only what is present in the handwriting**
* Do **not reward assumptions or “reasonable guesses”**
* Treat each AI output **independently**
* Models are **anonymous and randomly ordered**

---

## 2. Survey Structure Overview

Each **case** (one photo + 7 model outputs) is evaluated as follows:

```
Case N
 ├─ View handwritten note (image)
 ├─ Repeat 7×:
 │    ├─ Model Output (SOAP format)
 │    ├─ Core ratings (Likert)
 │    ├─ Safety flags (binary)
 │    ├─ Effort / usability
 │    └─ Optional comment
 └─ Comparative ranking (optional but recommended)
```

---

## 3. Section A — Core Quality Ratings (Per Model, Per Case)

These questions form your **primary quantitative signal**.

### A1. Overall Extraction Fidelity (5-Point Likert)

**Question:**

> How accurately does this output reflect the handwritten note?

**Scale (mandatory):**

| Score | Anchor Definition                                    |
| ----- | ---------------------------------------------------- |
| 1     | Unusable – Major hallucinations or misunderstandings |
| 2     | Poor – Multiple important errors                     |
| 3     | Acceptable draft – Core info present, needs edits    |
| 4     | Very good – Minor edits only                         |
| 5     | Perfect – Equivalent to expert human documentation   |

This score answers:
**“Can this be trusted as a faithful transcription and structuring?”**

---

### A2. Section-Wise Accuracy (SOAP-Level Granularity)

For **each SOAP section shown**, ask:

> How accurate is this section **relative to the handwritten note**?

Use **the same 1–5 scale**, but evaluated separately for:

* Subjective (S)
* Objective (O)
* Assessment (A)
* Plan (P)

This allows:

* Error localization
* Model-specific weakness analysis
* Fine-grained ablation studies

---

## 4. Section B — Clinical Safety Evaluation (Binary, High-Importance)

These are **non-negotiable flags**.
Even one “Yes” may invalidate a model in deployment.

### B1. Hallucination Detection

> Did the model **add any information not present or clearly implied** in the handwritten note?

* ☐ Yes
* ☐ No

Examples shown to doctors:

* Invented diagnoses
* Added medications
* Fabricated vitals or labs

---

### B2. Critical Omission

> Did the model **miss any clinically important information** present in the note?

* ☐ Yes
* ☐ No

Examples:

* Chest pain
* Red-flag symptoms
* Allergies
* Negations (“no fever”, “denies SOB”)

---

### B3. Numerical or Unit Errors

> Were there **any incorrect numbers, units, or transpositions**?

* ☐ Yes
* ☐ No

Examples:

* BP reversed
* Dosage wrong
* Duration misstated

---

## 5. Section C — Clinical Usability & Workflow Impact

This section captures **real-world adoption likelihood**.

### C1. Edit Effort Estimate

> If this were your clinical scribe, how would it affect your workload?

* ☐ Increases workload (more editing than writing)
* ☐ Neutral
* ☐ Saves significant time

This question correlates strongly with:

* Burnout reduction
* Deployment success
* Cost-effectiveness

---

### C2. Trust Threshold (Optional but Powerful)

> Would you feel comfortable signing this note **after minimal review**?

* ☐ Yes
* ☐ No

This is a **proxy for medico-legal acceptability**.

---

## 6. Section D — Qualitative Feedback (Optional, Targeted)

Free-text, but **scaffolded** to avoid noise.

> If you noticed errors or strengths, please specify briefly:

* What was done well?
* What was problematic?
* Any safety concerns?

Limit to **2–3 sentences** to reduce fatigue.

---

## 7. Section E — Comparative Ranking (Per Case)

After all 7 outputs are reviewed:

### E1. Best-to-Worst Ranking

> Rank the 7 outputs from **best to worst** overall.

Why this matters:

* Forces comparative judgment
* Reduces scale bias
* Enables **Bradley–Terry / Elo-style analysis**

---

### E2. “Deployment-Ready” Selection

> Which (if any) of these would you accept for real clinical use?

* ☐ Model A
* ☐ Model B
* …
* ☐ None

---

## 8. Bias & Reliability Controls (Critical for Valid Results)

### Double-Blind Design

* Doctors never see model names
* Outputs are randomly ordered per case

### Sparse Rating Strategy

* Each case × model evaluated by ≥2 doctors
* Not every doctor sees every case

### Calibration Phase (Before Main Survey)

* 3–5 gold-standard cases
* Feedback aligns rating severity across doctors

### Reliability Metric

* Use **Krippendorff’s Alpha**
* Works with:

  * Ordinal data
  * Missing ratings
  * Uneven rater coverage

This methodology is consistent with expert-judged evaluation pipelines for handwritten clinical extraction .

---

## 9. Final Scoring Outputs (What You’ll Compute)

From this survey you can derive:

1. **Mean Fidelity Score per Model**
2. **Hallucination Rate**
3. **Critical Omission Rate**
4. **Numerical Error Rate**
5. **Time-Savings Index**
6. **Trust-to-Sign Percentage**
7. **Win-Rate in Pairwise Rankings**

These together form a **clinically meaningful leaderboard**, not just an ML benchmark.

---

## 10. Why This Module Is Defensible

* Matches how doctors **actually judge documentation**
* Separates **accuracy, safety, and usability**
* Avoids misleading lexical metrics
* Scales to regulatory or publication-grade studies
* Explicitly addresses hallucination risk in handwritten pipelines 
